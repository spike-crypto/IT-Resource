# -*- coding: utf-8 -*-
"""IT_sup_Mod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ajYI59OldoWn1nP_h4bkklGC9k94OzE0

## Refine the prompt (if necessary)

### Subtask:
Refine the prompt to improve the accuracy and consistency of the model's output.

**Reasoning**:
I need to refine the prompt to address the JSON decoding error observed in the previous step. The error was caused by malformed JSON output from the model. I will modify the system message to provide clearer instructions on generating valid JSON, specifically addressing potential issues with special characters and ensuring the `auto_approval` field is either "Yes" or "No". I will then re-run the test queries to confirm the fix.
"""

import os
import json
from huggingface_hub import InferenceClient

# Refine the classify_it_request_new function with an improved prompt
def classify_it_request_new(user_query):
    """
    Classifies an IT support request and extracts structured information
    using a generative model with a refined prompt for better JSON output.
    """
    # Get the Hugging Face token from an environment variable for security
    hf_token = os.environ.get("HF_TOKEN")
    if not hf_token:
        return {"error": "Hugging Face API token not configured on the server."}

    prompt = """
    You are an intelligent IT Support Assistant helping employees raise and classify their IT-related issues.

    Analyze the following employee message and extract structured information needed to automate internal support workflows.

    Your task is to identify:
    - What the user is requesting (laptop, password reset, access issue, etc.)
    - The urgency of the request (High, Medium, Low)
    - Whether the request can be auto-approved or not ("Yes" or "No")
    - A short summary of the issue
    - Which department should handle this (IT Desk, Security, HR, etc.)
    - A polite message to respond to the user. Ensure this message is plain text and does not contain special characters that would break JSON formatting, such as unescaped backslashes or quotes within the string.

    Return the output strictly as a valid JSON object with these keys:
    {
      "request_type": "",
      "description": "",
      "urgency": "",
      "auto_approval": "",
      "response_message": "",
      "route_to": ""
    }
    Ensure the JSON is valid and properly formatted.

    Here is the user message:
    """

    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": user_query}
    ]

    client = InferenceClient(model="mistralai/Mistral-7B-Instruct-v0.2", token=hf_token)

    # Use chat_completion as the model supports conversational task
    response = client.chat_completion(messages, max_tokens=250)

    # Extract and parse the JSON from the response
    json_string = response.choices[0].message.content.strip() # Strip whitespace

    # Attempt to parse the JSON string. Handle potential errors if the output is not perfectly formed JSON.
    try:
        # Find the first and last curly braces to isolate the JSON object
        start_index = json_string.find('{')
        end_index = json_string.rfind('}')
        if start_index != -1 and end_index != -1:
            json_string_cleaned = json_string[start_index : end_index + 1]
            output_json = json.loads(json_string_cleaned)
        else:
             # If cannot find JSON braces, return error
            output_json = {"error": "Could not find valid JSON object in model response", "raw_response": json_string}

    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        print(f"Received raw response: {json_string}")
        # Return an empty dictionary or a dictionary with error information
        output_json = {"error": "Failed to parse JSON from model response", "raw_response": json_string}

    return output_json

# Test the new system with the variety of IT support queries again
# Note: To run this locally now, you would need to set the environment variable first.
# This test will likely fail in Colab unless you set the environment variable.
# test_queries_new = [
#     "My laptop is broken but i have call within hour with client.", # Permissions Access
#     "my collegue is beathing me ",
# ]

# for query in test_queries_new:
#     print(f"Original Query: {query}")
#     json_output = classify_it_request_new(query)
#     print("Classified Output:")
#     print(json.dumps(json_output, indent=4))
#     print("-" * 40)

"""### 5. Creating an API with Flask

To integrate this classification logic with your SAP CAP backend, you can create a simple REST API using a web framework like Flask. Your CAP application would then call this API.

First, you need to install Flask.
"""

# pip install flask  # Install Flask if not already installed

"""Now, here is a simple Flask application that exposes our `classify_it_request_new` function as an API endpoint.

**Note:** Running this cell in Colab will start a web server, but it won't be publicly accessible. This code is for demonstration purposes to show you how you would structure the API for deployment on a server.
"""

from flask import Flask, request, jsonify
import json

# --- Assume classify_it_request_new is defined as in the previous cells ---
# You would copy the full function definition here. For brevity, I am reusing it.

# Initialize the Flask application
app = Flask(__name__)

# Health check endpoint
@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint for deployment monitoring."""
    return jsonify({"status": "healthy", "service": "IT Support Classifier"}), 200

# Define the API endpoint for classification
@app.route('/classify', methods=['POST'])
def classify_endpoint():
    """
    API endpoint to classify an IT support request.
    Expects a JSON payload with a "query" key.
    """
    # Get the user query from the request body
    data = request.get_json()
    if not data or 'query' not in data:
        return jsonify({"error": "Invalid input, 'query' key is required."}), 400

    user_query = data['query']

    # Validate and sanitize user input
    if not user_query or not isinstance(user_query, str):
        return jsonify({"error": "Query must be a non-empty string."}), 400

    # Trim whitespace and limit length to prevent abuse
    user_query = user_query.strip()
    if len(user_query) > 1000:  # Reasonable limit for IT support queries
        return jsonify({"error": "Query too long. Maximum 1000 characters allowed."}), 400

    if len(user_query) < 5:  # Minimum meaningful query length
        return jsonify({"error": "Query too short. Please provide more details."}), 400

    # Call the classification function
    try:
        classified_output = classify_it_request_new(user_query)

        # Check if the classification returned an error
        if "error" in classified_output:
            return jsonify(classified_output), 500

        # The function already returns a dictionary, so we can directly jsonify it
        return jsonify(classified_output)
    except Exception as e:
        # Handle potential errors during classification
        return jsonify({"error": "An error occurred during classification.", "details": str(e)}), 500

# This part allows you to run the app directly
# In a real deployment, you would use a production server like Gunicorn
if __name__ == '__main__':
    # Check if token is available for testing
    if os.environ.get("HF_TOKEN"):
        test_query = "My laptop screen is flickering and I think it might be a hardware issue. It's quite urgent as I have a presentation tomorrow."
        print("Running test with available token...")
        classified_output = classify_it_request_new(test_query)
        print("\nClassified Output:")
        print(json.dumps(classified_output, indent=4))
    else:
        print("HF_TOKEN not found. Set it as environment variable for testing.")

    # Start Flask app for deployment
    print("\nStarting Flask application...")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)), debug=False)

# --- Environment variable should be set on the deployment server ---
# For GitHub deployment, set HF_TOKEN as a repository secret or environment variable
# The token will be automatically available during deployment